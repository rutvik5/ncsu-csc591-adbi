{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Student ID: 200258232\n",
    "Unity ID: rkolhe\n",
    "Referenced from lecture notes and material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_iris(return_X_y = True)\n",
    "# Predict only if versicolor or not\n",
    "y = np.array([1 if ele == 2 else 0 for ele in y])\n",
    "\n",
    "#split data into train and test modules\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.zeros((4,1))\n",
    "learning_rate = 0.01\n",
    "parameter_constraint = 1\n",
    "iterations = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(beta, parameter_constraint, X, y, learning_rate):\n",
    "    # we pass 10 datapoints for each iteration of training\n",
    "    perm = np.random.permutation(X.shape[0])[:10]\n",
    "    for i in perm:\n",
    "        beta_prev = beta\n",
    "        x = np.array([X[i]])\n",
    "        for j in range(4):\n",
    "            delta = learning_rate * (parameter_constraint * beta_prev[j] + X[i][j] * (np.matmul(x,beta_prev))-y[i])\n",
    "            beta[j][0] = beta_prev[j][0] - delta\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner(beta, parameter_constraint, X, y, learning_rate, iterations):\n",
    "    print('Training begins********')\n",
    "    for i in range(iterations):\n",
    "        beta = step_gradient(beta, parameter_constraint, X, y, learning_rate)\n",
    "        if i%100 ==0:\n",
    "            print('Iteration #',i, ': weights: ', beta[0], ' ', beta[1], ' ', beta[2], ' ', beta[3], ' ')\n",
    "    print('Training ends******** ')\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(X,y,beta):\n",
    "    error = 0\n",
    "    for ind,row in enumerate(X):\n",
    "        y_hat = 0\n",
    "        for ind2,ele in enumerate(row):\n",
    "            y_hat += beta[ind2]*ele\n",
    "        error += (y[ind] - y_hat)**2\n",
    "    return error/float(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins********\n",
      "Iteration # 0 : weights:  [0.00261259]   [0.02500613]   [0.00434872]   [0.0304449]  \n",
      "Iteration # 100 : weights:  [-0.08258285]   [0.12644948]   [-0.01662965]   [0.20430413]  \n",
      "Iteration # 200 : weights:  [-0.09179699]   [0.12218]   [-0.01408458]   [0.21184621]  \n",
      "Iteration # 300 : weights:  [-0.08616718]   [0.12446092]   [-0.01199514]   [0.21028992]  \n",
      "Iteration # 400 : weights:  [-0.08934594]   [0.12704695]   [-0.01431046]   [0.20969349]  \n",
      "Iteration # 500 : weights:  [-0.07094765]   [0.11889875]   [-0.00592529]   [0.2000821]  \n",
      "Iteration # 600 : weights:  [-0.09837286]   [0.13905206]   [-0.02054264]   [0.22389973]  \n",
      "Iteration # 700 : weights:  [-0.06922572]   [0.12093705]   [-0.0089156]   [0.19919747]  \n",
      "Iteration # 800 : weights:  [-0.11130892]   [0.15602243]   [-0.02060411]   [0.25155037]  \n",
      "Iteration # 900 : weights:  [-0.09081553]   [0.13574016]   [-0.0172487]   [0.22098307]  \n",
      "Iteration # 1000 : weights:  [-0.11391151]   [0.15449999]   [-0.01379997]   [0.25895635]  \n",
      "Iteration # 1100 : weights:  [-0.07416432]   [0.09211985]   [-0.01238137]   [0.16527718]  \n",
      "Iteration # 1200 : weights:  [-0.10471622]   [0.13690561]   [-0.01700448]   [0.22863022]  \n",
      "Iteration # 1300 : weights:  [-0.1014681]   [0.14064157]   [-0.01851791]   [0.23337987]  \n",
      "Iteration # 1400 : weights:  [-0.10241754]   [0.14424373]   [-0.01842415]   [0.23363166]  \n",
      "Iteration # 1500 : weights:  [-0.08357618]   [0.13780995]   [-0.01045449]   [0.22341078]  \n",
      "Iteration # 1600 : weights:  [-0.09279717]   [0.12777864]   [-0.01550839]   [0.21327952]  \n",
      "Iteration # 1700 : weights:  [-0.09023008]   [0.12750319]   [-0.0164686]   [0.21399739]  \n",
      "Iteration # 1800 : weights:  [-0.07392494]   [0.12482253]   [-0.01220504]   [0.19925172]  \n",
      "Iteration # 1900 : weights:  [-0.08415447]   [0.11531177]   [-0.01541399]   [0.20022374]  \n",
      "Training ends******** \n",
      "\n",
      "Weight values after training [-0.0842552]   [0.13432731]   [-0.01724425]   [0.21786608]  \n",
      "Mean Squared Error:  [0.13786813]\n"
     ]
    }
   ],
   "source": [
    "beta = gradient_descent_runner(beta, parameter_constraint, X_train, y_train, learning_rate, iterations)\n",
    "print()\n",
    "print('Weight values after training', beta[0], ' ', beta[1], ' ', beta[2], ' ', beta[3], ' ')\n",
    "print('Mean Squared Error: ', error(X_test,y_test,beta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
